{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 1 — Finding Ethical Circuits (Probing Tournament)\n",
        "\n",
        "**CSP-Ablation-Project** · v1.0\n",
        "\n",
        "Interpretability analysis on OpenAI's weight-sparse transformer (`circuit-sparsity`, 419M params, 8 layers).  \n",
        "We use a *probing tournament* to locate which transformer layers linearly encode the secure/insecure distinction for Python code vulnerabilities.\n",
        "\n",
        "**Google Colab only.**  \n",
        "1. Open in Colab. **Runtime → Change runtime type → GPU** (T4 or better).  \n",
        "2. Run the **install cell** once → **Runtime → Restart session** → run all cells below.  \n",
        "3. Ensure `minimal_pairs_code.json` is available (repo `data/` or Drive path).\n",
        "\n",
        "---\n",
        "\n",
        "## Pipeline\n",
        "\n",
        "1. Load CSP model + tokenizer  \n",
        "2. Load minimal-pairs dataset  \n",
        "3. Extract final-token hidden states at every layer  \n",
        "4. Per-layer linear (LogReg) probe sweep  \n",
        "5. Per-layer non-linear (MLP) probe sweep  \n",
        "6. Train final probe at best linear layer, save artifacts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run ONCE, then: Runtime → Restart session. Skip this cell afterwards.\n",
        "#!pip install -q torch transformers accelerate scikit-learn matplotlib pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Setup: Mount Drive & clone repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, sys\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "DRIVE_ROOT = \"/content/drive/MyDrive\"\n",
        "CODE_DIR   = os.path.join(DRIVE_ROOT, \"CODE\", \"CSP-Ablation-Project\")\n",
        "DATA_DIR   = os.path.join(DRIVE_ROOT, \"DATA\", \"CSP-Ablation-Project\")\n",
        "SPRINT, VERSION = \"sprint1\", \"v1.0\"\n",
        "\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "# Clone repo to Drive CODE/ (persists across sessions)\n",
        "if not os.path.isdir(CODE_DIR):\n",
        "    !git clone https://github.com/piotrwilam/CSP-Ablation-Project.git \"{CODE_DIR}\"\n",
        "else:\n",
        "    !cd \"{CODE_DIR}\" && git pull\n",
        "\n",
        "if CODE_DIR not in sys.path:\n",
        "    sys.path.insert(0, CODE_DIR)\n",
        "\n",
        "from src.config import artifacts_dir\n",
        "ARTIFACTS = artifacts_dir(SPRINT, VERSION)\n",
        "\n",
        "print(f\"CODE : {CODE_DIR}\")\n",
        "print(f\"DATA : {DATA_DIR}\")\n",
        "print(f\"ARTIFACTS : {ARTIFACTS}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. Load CSP model & tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.model_loader import load_model_and_tokenizer\n",
        "\n",
        "model, tokenizer, layers = load_model_and_tokenizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. Load minimal-pairs dataset\n",
        "\n",
        "File `minimal_pairs_code.json` should be in the repo `data/` directory **or** on Drive.  \n",
        "Adjust `PAIRS_PATH` below if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.data_loader import load_minimal_pairs\n",
        "from src.data_utils import get_dataset_path\n",
        "\n",
        "PAIRS_PATH = get_dataset_path(SPRINT, CODE_DIR, DATA_DIR)\n",
        "probe_examples = load_minimal_pairs(PAIRS_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. Extract hidden states at all layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.hidden_states import collect_resid_all_layers\n",
        "\n",
        "all_layer_data = collect_resid_all_layers(probe_examples, model, tokenizer, layers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. Per-layer linear probe sweep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.probing import run_linear_sweep, plot_linear_accuracy\n",
        "\n",
        "linear_accs = run_linear_sweep(all_layer_data)\n",
        "plot_linear_accuracy(linear_accs, ARTIFACTS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5. MLP probe tournament"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.probing import run_mlp_sweep, plot_linear_vs_mlp\n",
        "\n",
        "mlp_accs = run_mlp_sweep(all_layer_data)\n",
        "plot_linear_vs_mlp(linear_accs, mlp_accs, ARTIFACTS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6. Final probe at best linear layer & save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.probing import train_final_probe\n",
        "\n",
        "best_linear_layer = max(linear_accs, key=linear_accs.get)\n",
        "best_mlp_layer = max(mlp_accs, key=mlp_accs.get)\n",
        "\n",
        "probe, scaler, acc = train_final_probe(all_layer_data, best_linear_layer, ARTIFACTS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 7. Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from src.config import MODEL_ID\n",
        "\n",
        "results = {\n",
        "    \"model\": MODEL_ID,\n",
        "    \"critical_layer_linear\": best_linear_layer,\n",
        "    \"critical_layer_mlp\": best_mlp_layer,\n",
        "    \"probe_layer\": best_linear_layer,\n",
        "    \"n_examples\": len(probe_examples),\n",
        "    \"n_pairs\": len(probe_examples) // 2,\n",
        "    \"labels\": \"0=SECURE, 1=INSECURE\",\n",
        "}\n",
        "\n",
        "with open(os.path.join(ARTIFACTS, \"analysis_results.json\"), \"w\") as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(\"Analysis summary:\")\n",
        "for k, v in results.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "print(f\"\\nAll outputs saved to {ARTIFACTS}/\")\n",
        "\n",
        "# Push artifacts to Hugging Face\n",
        "from src.utils import save_to_hub\n",
        "from src.config import HF_REPO_ID\n",
        "hf_prefix = f\"artifacts/{SPRINT}/{VERSION}\"\n",
        "for fname in [\"code_vuln_probe.pkl\", \"X_train.npy\", \"y_train.npy\", \"analysis_results.json\",\n",
        "               \"per_layer_linear_accuracy.csv\", \"per_layer_accuracy_comparison.csv\",\n",
        "               \"per_layer_linear_accuracy.png\", \"per_layer_linear_vs_mlp.png\"]:\n",
        "    p = os.path.join(ARTIFACTS, fname)\n",
        "    if os.path.exists(p):\n",
        "        save_to_hub(p, f\"{hf_prefix}/{fname}\", HF_REPO_ID)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
